# Liquidation Agent Real-Time Implementation

## Overview

The liquidation agent has been completely refactored from a polling-based system to a real-time WebSocket-driven architecture that streams liquidation events from 5+ exchanges, processes them instantly, and stores data efficiently with dual storage (local Parquet + Supabase cloud).

## Architecture

### Components

1. **liquidation_websocket_manager.py** - WebSocket connections to multiple exchanges
2. **liquidation_storage.py** - Local Parquet-based storage with enhanced metrics
3. **liquidation_collector.py** - Background data collection script
4. **liquidation_agent.py** - Event-driven analysis and alerting
5. **cloud_database.py** - Cloud storage integration (Supabase)
6. **config.py** - Centralized configuration

### Data Flow

```
Exchange WebSockets â†’ liquidation_collector.py
                            â†“
                    Event Normalization & Enrichment
                            â†“
                    Buffer (10 sec batches)
                            â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                                       â†“
Local Parquet Storage                  Cloud Database
(24h retention)                        (Full history)
        â†“
liquidation_agent.py
        â†“
Rolling Statistics (15m/1h/4h windows)
        â†“
Threshold Detection
        â†“
AI Analysis (DeepSeek)
        â†“
BUY/SELL/NOTHING Signal
```

## Features

### Multi-Exchange Support

Streams from 5 exchanges simultaneously:
- **Binance** - Most reliable, highest volume
- **Bybit** - High liquidity, good API
- **OKX** - Global coverage
- **KuCoin** - Alternative feed
- **Bitfinex** - Additional data source

### Enhanced Data Collection

Collected data includes:
- **Core**: timestamp, exchange, symbol, side, price, quantity, USD value
- **Order Details**: type, time-in-force, average price
- **Market Context**: mark price, index price, price impact, spread
- **Cascade Indicators**: cumulative liquidations (1m/5m/15m), velocity, cascade score
- **Market Microstructure**: liquidity depth, imbalance ratio, volatility
- **Multi-Exchange**: concurrent exchanges, cross-exchange lag, dominant exchange

### Data Storage

**Local (Parquet Files)**:
- Daily partitioning: `liquidation_YYYYMMDD.parquet`
- 24-hour retention with automatic cleanup
- Snappy compression for efficient storage
- Fast columnar queries

**Cloud (Supabase)**:
- Full historical data
- Indexed for efficient querying
- Separate tables for raw events and analytics
- Automatic syncing every 60 seconds

## Configuration

All settings are in `src/config.py`:

```python
# Liquidation Agent Settings
LIQUIDATION_CHECK_INTERVAL = 120  # Backup polling interval (seconds)
LIQUIDATION_SYMBOLS = ['BTC', 'ETH', 'SOL']  # Symbols to track
LIQUIDATION_THRESHOLD = 0.5  # Multiplier for significant events
LIQUIDATION_COMPARISON_WINDOW = 15  # Time window in minutes
LIQUIDATION_LOCAL_RETENTION_HOURS = 24  # Keep 24 hours locally
LIQUIDATION_EXCHANGES = ['binance', 'bybit', 'okx', 'kucoin', 'bitfinex']

# Data Collection Settings
LIQUIDATION_BATCH_INTERVAL_SECONDS = 10  # Batch save frequency
LIQUIDATION_CLOUD_SYNC_INTERVAL_SECONDS = 60  # Cloud sync frequency
LIQUIDATION_CLOUD_SYNC_BATCH_SIZE = 100  # Events per sync batch

# AI Settings
LIQUIDATION_AI_MODEL = "deepseek-chat"
LIQUIDATION_AI_TEMPERATURE = 0.7
LIQUIDATION_AI_MAX_TOKENS = 150
```

## Usage

### Step 1: Start the Data Collector

The collector script runs continuously in the background, streaming data from all exchanges:

```bash
cd ai_crypto_agents
python src/scripts/data_processing/liquidation_collector.py
```

This will:
- Connect to all exchange WebSocket streams
- Normalize and enrich liquidation events
- Batch save to local Parquet files every 10 seconds
- Sync to Supabase cloud every 60 seconds
- Print statistics every minute

**Output Example:**
```
ğŸŒŠ Initializing Liquidation Collector...
ğŸ“Š Tracking symbols: BTC, ETH, SOL
ğŸŒ Monitoring exchanges: binance, bybit, okx, kucoin, bitfinex
âœ… Connected to binance
âœ… Connected to bybit
âœ… Connected to okx
...
ğŸ“Š LIQUIDATION COLLECTOR STATISTICS
â±ï¸  Uptime: 0:05:32
ğŸ“ˆ Total Events: 1,247
ğŸ’¾ Local Saves: 1,247
â˜ï¸  Cloud Saves: 1,200
```

### Step 2: Run the Liquidation Agent

The agent monitors the collected data and provides AI-powered trading signals:

```bash
cd ai_crypto_agents
python src/agents/liquidation_agent.py
```

This will:
- Monitor local Parquet files for significant liquidation spikes
- Calculate rolling statistics for each symbol
- Trigger AI analysis when thresholds are exceeded
- Generate BUY/SELL/NOTHING signals with confidence scores

**Output Example:**
```
ğŸŒŠ Luna the Liquidation Agent initialized!
ğŸ¯ Alerting on liquidation increases above +50% from previous
ğŸ“Š Analyzing symbols: BTC, ETH, SOL

ğŸŒŠ Liquidation Monitoring Cycle - 2025-11-10 15:30:00
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        ğŸŒ™ BTC Liquidation Data (15min)               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  LONGS:  $2,450,000.00 (125 events)                 â•‘
â•‘  SHORTS: $850,000.00 (42 events)                    â•‘
â•‘  Total Events: 167        Exchanges Active: 4       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸš¨ SIGNIFICANT LIQUIDATION SPIKE DETECTED for BTC!
ğŸ¤– Analyzing BTC liquidation spike with DeepSeek AI...

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        ğŸŒ™ BTC Liquidation Analysis ğŸ’¦                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Action: BUY                                           â•‘
â•‘  Confidence: 75%                                       â•‘
â•‘  Reason: Heavy long liquidations suggest bottoming     â•‘
â•‘  Long Change: +125.5%                                  â•‘
â•‘  Short Change: +15.2%                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

## API Reference

### LiquidationStorage

```python
from src.scripts.data_processing.liquidation_storage import LiquidationStorage

storage = LiquidationStorage()

# Save events
storage.save_liquidation_batch(events_list)

# Load history
history = storage.load_history(hours=24)

# Get aggregated stats
stats = storage.get_aggregated_stats(window_minutes=15, symbol='BTC')

# Query cascade events
cascades = storage.get_cascade_events(threshold_usd=1000000)

# Cross-exchange analysis
cross_ex_stats = storage.get_cross_exchange_stats(symbol='BTC', hours=1)

# Export for data sales
export_path = storage.export_for_sale(start_date, end_date, format='parquet')
```

### LiquidationWebSocketManager

```python
from src.scripts.shared_services.liquidation_websocket_manager import LiquidationWebSocketManager

manager = LiquidationWebSocketManager(symbols=['BTC', 'ETH', 'SOL'])

# Register callback
def on_liquidation(event):
    print(f"Liquidation: {event['symbol']} ${event['usd_value']:,.0f}")

manager.on_liquidation_event(on_liquidation)

# Start connections
await manager.connect_all_exchanges()

# Get statistics
stats = manager.get_statistics()
```

### Cloud Database

```python
from src.scripts.database.cloud_database import CloudDatabaseManager

db = CloudDatabaseManager()

# Save liquidation events
db.save_liquidation_events(events_list)

# Save analytics
db.save_liquidation_analytics(analytics_records)

# Query recent liquidations
recent = db.get_recent_liquidations(symbol='BTC', hours=24)
```

## Data Monetization

The enhanced liquidation data is valuable for:

1. **Trading Signal Generation** - High-quality liquidation cascade detection
2. **Market Making** - Microstructure data for spread optimization
3. **Risk Management** - Real-time cascade warnings
4. **Research** - Cross-exchange correlation analysis
5. **Data Products** - Historical liquidation datasets

### Export Data for Sale

```python
from datetime import datetime
from src.scripts.data_processing.liquidation_storage import LiquidationStorage

storage = LiquidationStorage()

# Export data range
start_date = datetime(2025, 11, 1)
end_date = datetime(2025, 11, 10)

# Export as Parquet (recommended)
export_path = storage.export_for_sale(start_date, end_date, format='parquet')

# Or CSV
export_path = storage.export_for_sale(start_date, end_date, format='csv')

# Or JSON
export_path = storage.export_for_sale(start_date, end_date, format='json')
```

## Performance

### Real-Time Latency
- **WebSocket Events**: Sub-second processing
- **Data Enrichment**: < 50ms per event
- **Local Storage**: < 100ms per batch
- **Cloud Sync**: < 2s per batch

### Data Volume
- **Events/Day**: ~50,000 - 200,000 (varies by market volatility)
- **Storage/Day**: ~50-200 MB (Parquet compressed)
- **Cloud Storage**: ~2-5 GB/month

### Resource Usage
- **CPU**: < 5% (idle), ~15% (high volume)
- **Memory**: ~200-500 MB
- **Network**: ~1-5 KB/s per exchange

## Troubleshooting

### No Data Being Collected

1. Check if liquidation_collector.py is running
2. Verify WebSocket connections: Check logs for connection status
3. Ensure DEEPSEEK_KEY is set in `.env`

### Missing Exchange Data

Some exchanges may require additional authentication:
- **KuCoin**: Requires token-based authentication for full functionality
- **Bitfinex**: May have regional restrictions

Solution: The system will continue working with available exchanges.

### High Memory Usage

If the event buffer grows too large:
1. Reduce `LIQUIDATION_BUFFER_SIZE` in config.py
2. Decrease `LIQUIDATION_BATCH_INTERVAL_SECONDS` for more frequent saves
3. Check that storage writes are succeeding

### Cloud Sync Failures

If cloud syncing fails:
1. Verify Supabase connection settings in `.env`
2. Check `POSTGRES_HOST`, `POSTGRES_USER`, `POSTGRES_PASSWORD`
3. Events will be queued and retried automatically
4. Local storage continues unaffected

## Monitoring

### Health Checks

The collector prints statistics every minute:
- Total events collected
- Events per exchange
- Events per symbol
- Connection status
- Save/sync success rates

### Log Files

Logs are written to:
- `src/logs/trading_system.log` - Main system log
- Console output - Real-time monitoring

## Comparison: Old vs New

| Feature | Old (Polling) | New (WebSocket) |
|---------|--------------|-----------------|
| **Latency** | 10 minutes | Sub-second |
| **Data Sources** | 1 API | 5 exchanges |
| **Symbols** | Variable | BTC, ETH, SOL |
| **Storage** | CSV | Parquet + Cloud |
| **Metrics** | Basic | 30+ fields |
| **Cascade Detection** | No | Yes |
| **Cross-Exchange** | No | Yes |
| **Data Quality** | Medium | High |
| **Reliability** | API-dependent | Multi-source |
| **Cost** | API fees | Free |

## Future Enhancements

Potential improvements:
1. **More Symbols**: Add top 20 cryptocurrencies
2. **Order Book Data**: Integrate depth-of-market
3. **Funding Rate Correlation**: Cross-reference with funding rates
4. **ML Models**: Predictive cascade detection
5. **Alert System**: Telegram/Discord notifications
6. **Dashboard**: Real-time visualization
7. **Backtesting**: Historical strategy testing

## Support

For issues or questions:
1. Check logs in `src/logs/trading_system.log`
2. Review configuration in `src/config.py`
3. Test individual components (storage, websocket, agent)
4. Verify environment variables in `.env`

## License

Built with love by Anarcho Capital ğŸš€
For internal use and data product sales.

